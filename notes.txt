To get the youBot in Gazebo and be able to control it:
* roslaunch youbot_object_grasp test.launch
* roslaunch youbot_control youbot_control.launch (from ~/youbot_tools_ws)

To see the youBot in rviz and manipulate the joints with sliders:
* roslaunch youbot_description youbot_rviz.launch (from ~/catkin_ws)

To see the youBot in rviz and manipulate the joints with interactive markers:
* roslaunch youbot_moveit demo.launch (from ~/winter_ws)

To set joints directly on youBot and see the tf:
* roslaunch youbot_driver_ros_interface youbot_driver.launch
* rosrun youbot_driver_ros_interface youbot_arm_test
* roslaunch youbot_description youbot_rviz.launch (from ~/hydrows)
* rosrun tf <some tf program>

To get the eband_local_planner to work in ROS Indigo (not necessary with ROS Hydro, specifically on youBot):
* Clone the package eband_local_planner from https://github.com/utexas-bwi/eband_local_planner.git
  - Modify CMakeLists.txt to add the line:  find_package(cmake_modules REQUIRED)
  - Modify package.xml to add:  
  	o <build_depend>cmake_modules</build_depend>
	o <run_depend>cmake_modules</run_depend>

The map being used is currently the map with four walls that is used in Gazebo.  This provides a nice empty space with no obstacles and still lets me fully use navstack.

Maybe try getting rid of the need for using eband_local_planner and just go with the default planner.  The only reason I am using it is because it was used on a project I copy/pasted.

Next Steps:
----------
- Test MoveIt code on the youBot
  * Put the MoveIt code on the youBot and make sure it can control the arm to the candle position, etc.
- Complete the set of pre-programmed arm angles
  * Need a starting position for above the block for grasping.

TO DO AT SOME POINT BEFORE PROJECT IS DONE:
------------------------------------------
- Can do dynamic calibration of the kinect's orientation.  Steps:
  1. Put arm into block search mode.
  2. Segment out the plane of the floor
  3. Determine the normal of the floor plane
  4. Use the normal of the floor plane and the known geometry and transformations of the youBot to determine the kinect's orientation.

Bonus Points:
------------
- Have visualizations that show how the robot is processing things
  o The point cloud display from the Kinect
  o The camera feed can be shown as another window
  o rviz can be used to show the processed point cloud and a cube overlaid in it to show where it thinks the block is
  o Show the planned path from navstack
  o Show the MoveIt! planning in rviz before moving the arm

Questions:
---------
- Is there a way to use MoveIt to specify only a position and don't care about the orientation?
- Is there a way to use MoveIt to specify a particular position and a suggested orientation and allow it to give me the closest orientation even if it is far off?
  * I want to specify a definite position and then say that I would really like to have a particular orientation but even if it ends up being far off, oh well, just give me the closest orientation.

From meeting:
------------
High weight on position, high weight on two of orientations, low weight on final orientation
Weighted velocity controller, similar to the damped least squares of last quarter, but that weighted them all evenly - should work a lot of the time if the position relative to the base is correct
Or could use my "scripted" method - setting the arm in "search" position, adjust the base, adjust yaw of gripper, move arm down to grasp

For target location picking:
---------------------------
- position = /youbot/odom + block_pose
  * Subscribe to odom and use that in arm_control.cpp before publishing a position.
  * Wrap the odom data inside a mutex

For navstack:
------------
- Try getting rid of all of the maps and just be able to use move_base to define goals and let it control things.  Would like to avoid needing the laser...

How target positioning works:
----------------------------
- Object is found in camera using PCL
- Transform from camera to object is returned by PCL
- Transform from camera to object is published by the node handling segmentation.
- Base control node picks up the published transform.
- Base control node uses tf to figure out where the block is relative to the base.
- Base control node subscribes to /youbot/odom to get odometry data relative to map frame.
- Base control node uses adds odom (x,y) data to the block's (x,y) tf to get map coords.
- Base control node publishes a goal to move_base_simple/goal.

How navstack stuff works:
------------------------
- move_base handles actually moving the youBot.
- odom_tf_pub node generates a transform between odom and base_link frames.
- amcl node maintains the transform between odom and map frames and updates positioning every now and then.


